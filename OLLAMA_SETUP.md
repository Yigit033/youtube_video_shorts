# ğŸ¤– Ollama Kurulum KÄ±lavuzu

## ğŸ¯ Neden Ollama?

### âœ… Avantajlar:
```
âœ… Tamamen Ã¼cretsiz
âœ… Lokal Ã§alÄ±ÅŸÄ±r (internet gerekmez)
âœ… Ã‡ok kaliteli scriptler (+300%)
âœ… Context awareness
âœ… Llama 3, Mistral, Gemma modelleri
âœ… API limiti yok
```

### âŒ HuggingFace Sorunu:
```
âŒ 410 error (deprecated models)
âŒ API limitleri
âŒ Ä°nternet gerekli
âŒ YavaÅŸ yanÄ±t
```

---

## ğŸš€ Ollama Kurulumu (Windows)

### 1. **Download ve Kurulum:**
```powershell
# Ollama'yÄ± indir
# https://ollama.ai/download

# Installer'Ä± Ã§alÄ±ÅŸtÄ±r
# OllamaSetup.exe

# Kurulum tamamlandÄ±!
```

### 2. **Ollama'yÄ± BaÅŸlat:**
```powershell
# Yeni terminal aÃ§
ollama serve

# Ã‡Ä±ktÄ±:
# Listening on 127.0.0.1:11434
```

### 3. **Model Ä°ndir:**
```powershell
# Yeni terminal aÃ§ (ollama serve Ã§alÄ±ÅŸÄ±rken)

# Llama 3 (Ã–nerilen) - 4.7GB
ollama pull llama3:8b

# Alternatifler:
# ollama pull mistral:7b    # 4.1GB
# ollama pull gemma:7b      # 5.0GB
# ollama pull llama3:70b    # 40GB (Ã§ok gÃ¼Ã§lÃ¼ ama yavaÅŸ)
```

### 4. **Test:**
```powershell
# Test et
ollama run llama3:8b "Write a short story about technology"

# BaÅŸarÄ±lÄ±! AI yanÄ±t verdi
```

---

## ğŸ›ï¸ .env KonfigÃ¼rasyonu

### Ollama'yÄ± Aktif Et:
```env
# Local AI kullan
USE_LOCAL_AI=true

# Ollama model
OLLAMA_MODEL=llama3:8b

# Ollama URL (default)
OLLAMA_URL=http://localhost:11434

# HuggingFace fallback (opsiyonel)
HUGGINGFACE_API_KEY=your_key_here
```

---

## ğŸ¤– Mevcut Modeller

### Llama 3 (Ã–nerilen) â­â­â­â­â­
```
Model: llama3:8b
Boyut: 4.7GB
Kalite: MÃ¼kemmel
HÄ±z: HÄ±zlÄ± (2-5 saniye)
Context: 8K tokens
```

### Mistral â­â­â­â­
```
Model: mistral:7b
Boyut: 4.1GB
Kalite: Ã‡ok iyi
HÄ±z: Ã‡ok hÄ±zlÄ± (1-3 saniye)
Context: 8K tokens
```

### Gemma â­â­â­â­
```
Model: gemma:7b
Boyut: 5.0GB
Kalite: Ä°yi
HÄ±z: HÄ±zlÄ± (2-4 saniye)
Context: 8K tokens
```

### Llama 3 70B â­â­â­â­â­
```
Model: llama3:70b
Boyut: 40GB
Kalite: OlaÄŸanÃ¼stÃ¼
HÄ±z: YavaÅŸ (10-30 saniye)
Context: 8K tokens
Gereksinim: 32GB+ RAM
```

---

## ğŸ“Š Performans KarÅŸÄ±laÅŸtÄ±rmasÄ±

### AI Script Kalitesi:

| AI | Kalite | HÄ±z | Maliyet | Ä°nternet |
|----|--------|-----|---------|----------|
| **Ollama (Llama3)** | â­â­â­â­â­ | 2-5s | $0 | âŒ |
| HuggingFace | â­â­â­â­ | 5-10s | $0 | âœ… |
| Templates | â­â­â­ | <1s | $0 | âŒ |

---

## ğŸ¯ KullanÄ±m

### Otomatik KullanÄ±m:
```bash
# 1. Ollama'yÄ± baÅŸlat (bir kere)
ollama serve

# 2. Platform'u baÅŸlat (yeni terminal)
npm start

# âœ… [AI] Using local Ollama...
# âœ… [AI] Script generated successfully!
```

### Manuel Test:
```javascript
const aiService = require('./services/ai');

// Test
const script = await aiService.generateScript('technology');
// âœ… Ollama kullanÄ±lacak
```

---

## ğŸ”§ Sorun Giderme

### Problem: "ECONNREFUSED 127.0.0.1:11434"
```powershell
# Ollama Ã§alÄ±ÅŸmÄ±yor
# Ã‡Ã¶zÃ¼m: Ollama'yÄ± baÅŸlat
ollama serve

# Yeni terminal'de platform'u baÅŸlat
npm start
```

### Problem: "Model not found"
```powershell
# Model indirilmemiÅŸ
# Ã‡Ã¶zÃ¼m: Model'i indir
ollama pull llama3:8b

# Model listesi
ollama list
```

### Problem: "Out of memory"
```powershell
# RAM yetersiz
# Ã‡Ã¶zÃ¼m: Daha kÃ¼Ã§Ã¼k model kullan

# 8GB RAM â†’ llama3:8b
# 16GB RAM â†’ mistral:7b veya llama3:8b
# 32GB+ RAM â†’ llama3:70b
```

### Problem: "Slow response"
```powershell
# Model Ã§ok bÃ¼yÃ¼k
# Ã‡Ã¶zÃ¼m: Daha kÃ¼Ã§Ã¼k model

# llama3:70b â†’ llama3:8b
# veya
# mistral:7b (en hÄ±zlÄ±)
```

---

## ğŸ“ˆ Beklenen SonuÃ§lar

### Ã–ncesi (Templates):
```
ğŸ“ Script kalitesi: â­â­â­
ğŸ¯ Context awareness: Yok
âš¡ HÄ±z: <1 saniye
ğŸ’¡ YaratÄ±cÄ±lÄ±k: DÃ¼ÅŸÃ¼k
```

### SonrasÄ± (Ollama):
```
ğŸ“ Script kalitesi: â­â­â­â­â­
ğŸ¯ Context awareness: MÃ¼kemmel
âš¡ HÄ±z: 2-5 saniye
ğŸ’¡ YaratÄ±cÄ±lÄ±k: YÃ¼ksek
```

---

## ğŸ’¡ Pro Tipler

### 1. **Ollama'yÄ± Arka Planda Ã‡alÄ±ÅŸtÄ±r:**
```powershell
# Windows Service olarak kur
# Otomatik baÅŸlasÄ±n

# Veya Task Scheduler ile
# BaÅŸlangÄ±Ã§ta otomatik baÅŸlat
```

### 2. **FarklÄ± Modeller Dene:**
```env
# HÄ±z Ã¶ncelikli
OLLAMA_MODEL=mistral:7b

# Kalite Ã¶ncelikli
OLLAMA_MODEL=llama3:8b

# En iyi kalite (gÃ¼Ã§lÃ¼ PC)
OLLAMA_MODEL=llama3:70b
```

### 3. **Fallback Chain:**
```
Ollama â†’ HuggingFace â†’ Templates
```

---

## ğŸ‰ Kurulum TamamlandÄ±!

### Test Et:
```bash
# Terminal 1: Ollama
ollama serve

# Terminal 2: Platform
npm start

# Dashboard: http://localhost:3000
# Topic: "artificial intelligence"
# Count: 1
```

### Beklenen Ã‡Ä±ktÄ±:
```
ğŸ¤– [AI] Using local Ollama...
ğŸ¤– Ollama: Generating script with llama3:8b...
âœ… [AI] Script generated successfully!
ğŸ“ Script quality: Excellent
âš¡ Generation time: 3.2s
```

---

## ğŸš€ SONUÃ‡

**Ollama = En Ä°yi AI Ã‡Ã¶zÃ¼mÃ¼!**

- âœ… Tamamen Ã¼cretsiz
- âœ… Ã‡ok kaliteli scriptler
- âœ… Lokal Ã§alÄ±ÅŸÄ±r
- âœ… API limiti yok
- âœ… Kolay kurulum

**Åimdi kur ve AI gÃ¼cÃ¼nÃ¼ kullan! ğŸ¤–âœ¨**

---

## ğŸ“š Ek Kaynaklar

### Ollama KomutlarÄ±:
```powershell
# Model listesi
ollama list

# Model sil
ollama rm llama3:8b

# Model bilgisi
ollama show llama3:8b

# TÃ¼m modeller
ollama pull --help
```

### Ã–nerilen Setup:
```
ğŸ’» 8GB RAM: mistral:7b
ğŸ’» 16GB RAM: llama3:8b
ğŸ’» 32GB+ RAM: llama3:70b
```

**BaÅŸarÄ±lar! ğŸš€**
